{"cells":[{"cell_type":"markdown","metadata":{},"source":["### **Step 1: Import packages**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":836,"status":"ok","timestamp":1646967145981,"user":{"displayName":"Wenyao Arthur Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFqa89Tv04aFkrminp8fj7TUZGTTc9H5ihxfIQ=s64","userId":"01122744164549624887"},"user_tz":-780},"id":"REGo9OXXGKl0"},"outputs":[],"source":["import pandas as pd\n","import requests\n","import time\n","import html\n","import datetime"]},{"cell_type":"markdown","metadata":{},"source":["### **Step 2: Get API key and channel id info**"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1646968310571,"user":{"displayName":"Wenyao Arthur Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFqa89Tv04aFkrminp8fj7TUZGTTc9H5ihxfIQ=s64","userId":"01122744164549624887"},"user_tz":-780},"id":"ehe07gI6GZ6j"},"outputs":[],"source":["# API key\n","api_key = '*****' # concealed for privacy reasons\n","# channel id for 'Food Wishes'\n","channel_id = 'UCRIZtPl9nb9RiXc9btSTQNw'"]},{"cell_type":"markdown","metadata":{},"source":["### **Step 3: Define functions to acquire stats for videos on the channel**"]},{"cell_type":"markdown","metadata":{},"source":["##### **Step 3.1: Define the function to acquire stats for a particular video**\n","\n","- YouTube videos are identified by video ids.\n","\n","- We need to provide the video id to see those stats.\n","\n","- Youtube provides access to view counts, like counts, and comment counts via the API.\n","\n","- The access to dislike counts has been discontinued."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_video_stats(video_id):\n","    video_stats_url = 'https://www.googleapis.com/youtube/v3/videos?id='+video_id+\\\n","                    '&part=statistics&key='+api_key\n","    response_video_stats = requests.get(video_stats_url).json()\n","    view_count = response_video_stats['items'][0]['statistics']['viewCount']\n","    like_count = response_video_stats['items'][0]['statistics']['likeCount']\n","    comment_count = response_video_stats['items'][0]['statistics']['commentCount']\n","    return view_count, like_count, comment_count"]},{"cell_type":"markdown","metadata":{},"source":["##### **Step 3.2: Define the function to consolidate the stats of all videos into a dataframe**\n","\n","1. The dataframe will record video ids, video title, upload dates, and the three counts.\n","\n","1. The dataframe will collect all stats in the period of start year (1 Jan) -> end year (31 Dec).\n","\n","1. A start year is always needed. Unless the end year is specifically given, its default is set to the current year.\n","\n","1. API queries will be made in half-yearly batches to avoid Google's stipulation of maximum 500-entry limit per query."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_channel_data(start_year, end_year=datetime.datetime.now().year):\n","    # create an empty dataframe to store video info, provide column names\n","    df = pd.DataFrame(columns=['video_id', 'video_title', 'upload_date',\\\n","                               'view_count', 'like_count', 'comment_count'])\n","    # create a list for time periods\n","    dates = list()\n","    for date in range(start_year, end_year+1):\n","        dates.append(f'{date}-01-01,{date}-12-31')\n","    # as Google imposes a 500-entry limit on one single API query,\n","    # we need to go through smaller time periods one at a time\n","    for date in dates:\n","        after, before = date.split(',')\n","        after_date = after+'T00:00:00Z'\n","        before_date = before+'T00:00:00Z'\n","        # if this is the first time running requests  \n","        # define first-time url\n","        url_initial = 'https://www.googleapis.com/youtube/v3/search?key='+api_key+\\\n","        '&channelId='+channel_id+\\\n","        '&part=snippet,id&Order=date&maxResults=50&publishedBefore='+before_date+'&publishedAfter='+after_date\n","        # retrieve responses as JSON\n","        response = requests.get(url_initial).json()\n","        # iterate through responses to fetch video info\n","        time.sleep(1)\n","        for item in response['items']:\n","            if item['id']['kind'] == 'youtube#video':\n","                video_id = item['id']['videoId']\n","                video_title = html.unescape(item['snippet']['title']) # covert escaped chars back to original forms\n","                upload_date = item['snippet']['publishTime'].split('T')[0] # extract only the            \n","                # get stats\n","                view_count, like_count, comment_count = get_video_stats(video_id)\n","                # append the info to the dataframe\n","                df = df.append(\n","                    dict(\n","                        video_id = video_id, \n","                        video_title = video_title,\n","                        upload_date = upload_date,\n","                        view_count = view_count,\n","                        like_count = like_count,\n","                        comment_count = comment_count,\n","                    ), ignore_index=True\n","                )\n","        # when requests have been done once \n","        # firstly check if the period has less than 50 entries\n","        # if less than 50 entries, the 'nextPageToken' field won't appear in response\n","        try: \n","            response['nextPageToken']\n","        except KeyError:\n","            # print 'less than 50 entries' message\n","            print(f'{after} -> {before}: Less than 50 entries in this period. All entries collected.')\n","        else:\n","            # get page tokens\n","            page_token = response['nextPageToken']\n","            # traverse all pages until page tokens are no longer available in responses\n","            try:\n","                # when page tokens are showing\n","                while page_token != '':\n","                    url = 'https://www.googleapis.com/youtube/v3/search?key='+api_key+\\\n","                    '&channelId='+channel_id+\\\n","                    '&part=snippet,id&Order=date&maxResults=50&publishedBefore='+before_date+'&publishedAfter='+after_date+\\\n","                    '&pageToken='+page_token\n","                    new_response = requests.get(url).json()\n","                    page_token = new_response['nextPageToken']\n","                    time.sleep(1)       \n","                    # iterate through responses to fetch video info\n","                    for item in new_response['items']:\n","                        if item['id']['kind'] == 'youtube#video':\n","                            video_id = item['id']['videoId']\n","                            video_title = html.unescape(item['snippet']['title']) # covert escaped chars back to original forms\n","                            upload_date = item['snippet']['publishTime'].split('T')[0] # extract only the \n","                            # get stats\n","                            view_count, like_count, comment_count = get_video_stats(video_id)\n","                            # append the info to the dataframe\n","                            df = df.append(\n","                                dict(\n","                                    video_id = video_id, \n","                                    video_title = video_title,\n","                                    upload_date = upload_date,\n","                                    view_count = view_count,\n","                                    like_count = like_count,\n","                                    comment_count = comment_count,\n","                                ), ignore_index=True\n","                            )\n","            # when page tokens are not available\n","            except KeyError:\n","                # remove page token variable\n","                del response\n","                del page_token\n","                # if limit reached, print limit warning message\n","                if df.shape[0] == 500:\n","                    print('The 500-entry limit impose by Google is reached. Reduce the collection size.')\n","                # if traversal finished, print completion message\n","                else:\n","                    print(f'{after} -> {before}: All entries collected for this period.')\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["### **Step 4: Make API queries and collect data**"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2012-01-01 -> 2012-12-31: All entries collected for this period.\n","2013-01-01 -> 2013-12-31: All entries collected for this period.\n","2014-01-01 -> 2014-12-31: All entries collected for this period.\n","2015-01-01 -> 2015-12-31: All entries collected for this period.\n","2016-01-01 -> 2016-12-31: All entries collected for this period.\n","2017-01-01 -> 2017-12-31: All entries collected for this period.\n","2018-01-01 -> 2018-12-31: All entries collected for this period.\n","2019-01-01 -> 2019-12-31: All entries collected for this period.\n","2020-01-01 -> 2020-12-31: All entries collected for this period.\n","2021-01-01 -> 2021-12-31: All entries collected for this period.\n","2022-01-01 -> 2022-12-31: Less than 50 entries in this period. All entries collected.\n"]}],"source":["food_wishes_from2012 = get_channel_data(start_year=2012)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","      <th>video_title</th>\n","      <th>upload_date</th>\n","      <th>view_count</th>\n","      <th>like_count</th>\n","      <th>comment_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CQPLo8hECWg</td>\n","      <td>Twice Baked Potatoes -- How to Make Fancy Stuf...</td>\n","      <td>2012-12-15</td>\n","      <td>4194422</td>\n","      <td>47208</td>\n","      <td>1950</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TsrTU3CJn2c</td>\n","      <td>Irish Shepherd's Pie - Classic Shepherd Pie fo...</td>\n","      <td>2012-03-05</td>\n","      <td>2730601</td>\n","      <td>44227</td>\n","      <td>2843</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>wRtGM3f-UBc</td>\n","      <td>How to Flip Food in a Pan Like a Chef!</td>\n","      <td>2012-10-04</td>\n","      <td>2764300</td>\n","      <td>17836</td>\n","      <td>1383</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>QGAJokcwBXI</td>\n","      <td>Garlic Shrimp Recipe - Quick &amp; Easy Garlic Shrimp</td>\n","      <td>2012-02-17</td>\n","      <td>5094363</td>\n","      <td>55387</td>\n","      <td>1904</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ME9CM0zqubg</td>\n","      <td>Rosemary &amp; Honey Pull-Apart Dinner Rolls - Hol...</td>\n","      <td>2012-12-11</td>\n","      <td>606245</td>\n","      <td>10940</td>\n","      <td>599</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>pYhiIrlXY7I</td>\n","      <td>Hash Browns - Hash Browned Potato Recipe - Cla...</td>\n","      <td>2012-03-01</td>\n","      <td>2198124</td>\n","      <td>34812</td>\n","      <td>2023</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>fP45d9xxNt0</td>\n","      <td>Banana Bread Recipe - Chocolate Banana Nut Loaf</td>\n","      <td>2012-01-06</td>\n","      <td>881425</td>\n","      <td>16767</td>\n","      <td>1032</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>S4z2gmtUzHE</td>\n","      <td>Drunken Mussels Recipe - Mussels Steamed in a ...</td>\n","      <td>2012-05-21</td>\n","      <td>2224430</td>\n","      <td>27334</td>\n","      <td>1303</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>rYm2hHaN2i0</td>\n","      <td>Roasted Chicken Broth Recipe - Part 1 of How t...</td>\n","      <td>2012-02-22</td>\n","      <td>389670</td>\n","      <td>4512</td>\n","      <td>364</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>dKtySwuIZec</td>\n","      <td>Chicken &amp; Dumplings - Stewed Chicken with Thym...</td>\n","      <td>2012-05-29</td>\n","      <td>976791</td>\n","      <td>21267</td>\n","      <td>1025</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      video_id                                        video_title upload_date  \\\n","0  CQPLo8hECWg  Twice Baked Potatoes -- How to Make Fancy Stuf...  2012-12-15   \n","1  TsrTU3CJn2c  Irish Shepherd's Pie - Classic Shepherd Pie fo...  2012-03-05   \n","2  wRtGM3f-UBc             How to Flip Food in a Pan Like a Chef!  2012-10-04   \n","3  QGAJokcwBXI  Garlic Shrimp Recipe - Quick & Easy Garlic Shrimp  2012-02-17   \n","4  ME9CM0zqubg  Rosemary & Honey Pull-Apart Dinner Rolls - Hol...  2012-12-11   \n","5  pYhiIrlXY7I  Hash Browns - Hash Browned Potato Recipe - Cla...  2012-03-01   \n","6  fP45d9xxNt0    Banana Bread Recipe - Chocolate Banana Nut Loaf  2012-01-06   \n","7  S4z2gmtUzHE  Drunken Mussels Recipe - Mussels Steamed in a ...  2012-05-21   \n","8  rYm2hHaN2i0  Roasted Chicken Broth Recipe - Part 1 of How t...  2012-02-22   \n","9  dKtySwuIZec  Chicken & Dumplings - Stewed Chicken with Thym...  2012-05-29   \n","\n","  view_count like_count comment_count  \n","0    4194422      47208          1950  \n","1    2730601      44227          2843  \n","2    2764300      17836          1383  \n","3    5094363      55387          1904  \n","4     606245      10940           599  \n","5    2198124      34812          2023  \n","6     881425      16767          1032  \n","7    2224430      27334          1303  \n","8     389670       4512           364  \n","9     976791      21267          1025  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# see what the dataframe looks like\n","food_wishes_from2012.head(10)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["(794, 6)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# see how many entries have been collected\n","food_wishes_from2012.shape"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["video_id         object\n","video_title      object\n","upload_date      object\n","view_count       object\n","like_count       object\n","comment_count    object\n","dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# check the datatypes of all columns\n","food_wishes_from2012.dtypes"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# save the dataframe to a csv file\n","food_wishes_from2012.to_csv('food_wishes_from2007.csv')"]},{"cell_type":"markdown","metadata":{},"source":["### **Step 5: Conclusion**\n","\n","1. **We have collected 794 entries in the dataframe.**\n","\n","2. **The data are up-to-date as of Mar 12, 2022.**\n","\n","1. **The informaton collected was all stats since 2012 (the most recent 10 years).**\n","\n","1. **The dataframe is exported to a csv file for later processing.**"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPD37gfpMo3UqM6ZfdiO5Ab","name":"YouTube_analysis_Nate.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
